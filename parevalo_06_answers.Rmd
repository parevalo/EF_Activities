---
title: "Space state models"
author: "Paulo Arevalo"
date: "March 16, 2016"
output: html_document
---

```{r}
library(rjags)
```

```{r}
gflu = read.csv("http://www.google.org/flutrends/about/data/flu/us/data.txt",skip=11)
time = as.Date(gflu$Date)
y = gflu$Massachusetts
plot(time,y,type='l',ylab="Flu Index",lwd=2,log='y')
```

Assignment:
-----------

To look at how observation frequency affects data assimilation, convert 3 out of every 4 observations to NA (i.e. treat the data as approximately monthly) and refit the model. 

* Generate a time-series plot for the CI of x that includes the observations (as above). Use a different color and symbol to differentiate observations that were included in the model versus those that were converted to NA's.
* Compare the CI between the two runs.
* Generate a predicted (median) vs observed plot for the data points that were removed
* Comment on the accuracy and precision of the estimates.

Conversion to NA's

```{r}
nas <- sample(length(y), (y*0.75), replace = FALSE)
y.nas <- y
y.nas[nas] <- NA
```

New model

```{r}
RandomWalk2 = "
model{
  
  #### Data Model
  for(i in 1:n){
    y.nas[i] ~ dnorm(x[i],tau_obs)
  }
  
  #### Process Model
  for(i in 2:n){
    x[i]~dnorm(x[i-1],tau_add)
  }
  
  #### Priors
  x[1] ~ dnorm(x_ic,tau_ic)
  tau_obs  ~ dgamma(a_obs,r_obs)
  tau_add ~ dgamma(a_add,r_add)
}
"
```

Data re-definition

```{r}
data.nas <- list(y.nas=log(y.nas),n=length(y.nas),x_ic=log(1000),tau_ic=100,a_obs=1,r_obs=1,a_add=1,r_add=1)
```

Initial state

```{r}
nchain = 3
init2 <- list()
for(i in 1:nchain){
  y.samp2 = sample(y.nas,length(y.nas),replace=TRUE)
  init2[[i]] <- list(tau_add=1/var(diff(log(y.samp2))),tau_obs=5/var(log(y.samp2)))
}
```

Return JAGS model

```{r}
j.model2   <- jags.model (file = textConnection(RandomWalk2),
                             data = data.nas,
                             inits = init2,
                             n.chains = 3)
```

Assess convergence

```{r}
## burn-in
jags.out2   <- coda.samples (model = j.model2,
                            variable.names = c("tau_add","tau_obs"),
                                n.iter = 1000)
plot(jags.out2)
```

Larger sample 

```{r}
jags.out2   <- coda.samples (model = j.model2,
                            variable.names = c("x","tau_add","tau_obs"),
                                n.iter = 10000)
```

Plot - Stars are the NA values, crosses are the rest

```{r}
time.rng = c(1,length(time)) ## adjust to zoom in and out
ciEnvelope <- function(x,ylo,yhi,...){
  polygon(cbind(c(x, rev(x), x[1]), c(ylo, rev(yhi),
                                      ylo[1])), border = NA,...) 
}
out2 <- as.matrix(jags.out2)
ci2 <- apply(exp(out2[,3:ncol(out2)]),2,quantile,c(0.025,0.5,0.975))

plot(time,ci2[2,],type='n',ylim=range(y,na.rm=FALSE),ylab="Flu Index",log='y',xlim=time[time.rng])
## adjust x-axis label to be monthly if zoomed
if(diff(time.rng) < 100){ 
  axis.Date(1, at=seq(time[time.rng[1]],time[time.rng[2]],by='month'), format = "%Y-%m")
}
ciEnvelope(time,ci2[1,],ci2[3,],col="chartreuse3")
points(time,y.nas,pch="+",cex=1.2)
points(time[nas], y[nas], pch="*", cex=0.9)
legend('topleft', c("Included in the model", "Converted to NA's"), pch=c("+", "*"))
```

Posterior distributions 

```{r}
layout(matrix(c(1,2,3,3),2,2,byrow=TRUE))
hist(1/sqrt(out2[,1]),main=colnames(out2)[1])
hist(1/sqrt(out2[,2]),main=colnames(out2)[2])
plot(out2[,1],out2[,2],pch=".",xlab=colnames(out2)[1],ylab=colnames(out2)[2])
cor(out2[,1:2])
```

compare prediction vs observed

```{r}
plot(ci2[2,nas], y[nas], xlab="Predicted median value", ylab="Observed value", main="Observed vs Predicted value for the points that were removed")
abline(0,1)
```

Summary:

The confidence interval is generally wider for the run with the data converted into NA's compared to the original data. similarly, the posterior distributions for `tau_add and` and `tau_obs` display wider ranges, as expected given the big fraction of missing data. Even then, the model does a remarkable job of keeping the CI very low despite missing three fourths of the data. Accuracy and precision are very good, especially for the lower values where most of the data is concentrated anyway. Precision is reduced further up, where observed values tend to be much higher than the predicted ones. This is expected, though, as some of the highest values (like 2013) tend to be way above the mean.

Extra Credit:
-------------

Return to the original data and instead of removing 3/4 of the data remove the last 40 observations (convert to NA) and refit the model to make a forecast for this period

* Generate a time-series plot for the CI of x that includes the observations (as above but zoom the plot on the last ~80 observations). Use a different color and symbol to differentiate observations that were included in the model versus those that were converted to NA's.
* Comment on how well the random walk model performed (both accuracy and precision) and how it might be modified to improve both these criteria.

Conversion to NA's

```{r}
y.sub <- y
y.sub[580:620] <- NA
```

New model

```{r}
RandomWalk3 = "
model{
  
  #### Data Model
  for(i in 1:n){
    y.sub[i] ~ dnorm(x[i],tau_obs)
  }
  
  #### Process Model
  for(i in 2:n){
    x[i]~dnorm(x[i-1],tau_add)
  }
  
  #### Priors
  x[1] ~ dnorm(x_ic,tau_ic)
  tau_obs  ~ dgamma(a_obs,r_obs)
  tau_add ~ dgamma(a_add,r_add)
}
"
```

Data re-definition

```{r}
data.sub <- list(y.sub=log(y.sub),n=length(y.sub),x_ic=log(1000),tau_ic=100,a_obs=1,r_obs=1,a_add=1,r_add=1)
```

Initial state

```{r}
nchain = 3
init3 <- list()
for(i in 1:nchain){
  y.samp3 = sample(y.sub,length(y.sub),replace=TRUE)
  init3[[i]] <- list(tau_add=1/var(diff(log(y.samp3))),tau_obs=5/var(log(y.samp3)))
}
```

Return JAGS model

```{r}
j.model3   <- jags.model (file = textConnection(RandomWalk3),
                             data = data.sub,
                             inits = init3,
                             n.chains = 3)
```

Assess convergence

```{r}
## burn-in
jags.out3   <- coda.samples (model = j.model3,
                            variable.names = c("tau_add","tau_obs"),
                                n.iter = 1000)
plot(jags.out3)
```

Larger sample 

```{r}
jags.out3   <- coda.samples (model = j.model3,
                            variable.names = c("x","tau_add","tau_obs"),
                                n.iter = 10000)
```

Plot - Stars are the NA values, crosses are the rest

```{r}
time.rng = c(540,length(time)) ## adjust to zoom in and out
ciEnvelope <- function(x,ylo,yhi,...){
  polygon(cbind(c(x, rev(x), x[1]), c(ylo, rev(yhi),
                                      ylo[1])), border = NA,...) 
}
out3 <- as.matrix(jags.out3)
ci3 <- apply(exp(out3[,3:ncol(out3)]),2,quantile,c(0.025,0.5,0.975))

plot(time,ci3[2,],type='n',ylim=range(y,na.rm=FALSE),ylab="Flu Index",log='y',xlim=time[time.rng])
## adjust x-axis label to be monthly if zoomed
if(diff(time.rng) < 100){ 
  axis.Date(1, at=seq(time[time.rng[1]],time[time.rng[2]],by='month'), format = "%Y-%m")
}
ciEnvelope(time,ci3[1,],ci3[3,],col="chartreuse4")
points(time,y.sub,pch="+",cex=1)
points(time[580:620], y[580:620], pch="*", cex=0.9)
legend('topleft', c("Included in the model", "Converted to NA's"), pch=c("+", "*"))
```

compare prediction vs observed

```{r}
plot(ci3[2,580:620], y[580:620], xlab="Predicted median value", ylab="Observed value", main="Observed vs Predicted value for the points that were removed")
abline(0,1)
```

Summary:

The model is not very good at forecasting the last 40 values of the series in a very accurate manner, especially towards the most recent dates, which is not surprising given that we're just adding some random noise and not providing sufficiently informative priors that could provide a tighter credible interval. Precision behaves in a similar way, becoming smaller with time. Specifying priors on expected seasonality of flu occurrences (which will vary on location and some environmental variables like precipitation or temperature) will probably enhance the performance of the model. 
